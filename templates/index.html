<!--<a href="/exec" class="button"> This link will invoke Parse method </a> -->
<html>
<body>
<style>
@import url(https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css);
@import url(https://fonts.googleapis.com/css?family=Comfortaa);

body {
  font-family: 'Comfortaa', cursive;
  background: #353535;
  color: #FFF;
}
.jumbotron h1 {
  color: #353535;
}


footer {
  margin-bottom: 0 !important;
  margin-top: 80px;
}
footer p {
  margin: 0;
  padding: 0;
}
span.icon {
  margin: 0 5px;
  color: #D64541;
}
h2 {
  color: #BDC3C7;
  text-transform: uppercase;
  letter-spacing: 1px;
}
.mrng-60-top {
  margin-top: 60px;
}
/* Global Button Styles */
a.animated-button:link, a.animated-button:visited {
  position: relative;
  display: block;
  margin: 30px auto 0;
  padding: 14px 15px;
  color: #fff;
  font-size:14px;
  font-weight: bold;
  text-align: center;
  text-decoration: none;
  text-transform: uppercase;
  overflow: hidden;
  letter-spacing: .08em;
  border-radius: 0;
  text-shadow: 0 0 1px rgba(0, 0, 0, 0.2), 0 1px 0 rgba(0, 0, 0, 0.2);
  -webkit-transition: all 1s ease;
  -moz-transition: all 1s ease;
  -o-transition: all 1s ease;
  transition: all 1s ease;
}
a.animated-button:link:after, a.animated-button:visited:after {
  content: "";
  position: absolute;
  height: 0%;
  left: 50%;
  top: 50%;
  width: 150%;
  z-index: -1;
  -webkit-transition: all 0.75s ease 0s;
  -moz-transition: all 0.75s ease 0s;
  -o-transition: all 0.75s ease 0s;
  transition: all 0.75s ease 0s;
}
a.animated-button:link:hover, a.animated-button:visited:hover {
  color: #FFF;
  text-shadow: none;
}
a.animated-button:link:hover:after, a.animated-button:visited:hover:after {
  height: 450%;
}
a.animated-button:link, a.animated-button:visited {
  position: relative;
  display: block;
  margin: 30px auto 0;
  padding: 14px 15px;
  color: #fff;
  font-size:14px;
  border-radius: 0;
  font-weight: bold;
  text-align: center;
  text-decoration: none;
  text-transform: uppercase;
  overflow: hidden;
  letter-spacing: .08em;
  text-shadow: 0 0 1px rgba(0, 0, 0, 0.2), 0 1px 0 rgba(0, 0, 0, 0.2);
  -webkit-transition: all 1s ease;
  -moz-transition: all 1s ease;
  -o-transition: all 1s ease;
  transition: all 1s ease;
}

/* Victoria Buttons */

a.animated-button.victoria-one {
  border: 2px solid #D24D57;
}
a.animated-button.victoria-one:after {
  background: #D24D57;
  -moz-transform: translateX(-50%) translateY(-50%) rotate(-25deg);
  -ms-transform: translateX(-50%) translateY(-50%) rotate(-25deg);
  -webkit-transform: translateX(-50%) translateY(-50%) rotate(-25deg);
  transform: translateX(-50%) translateY(-50%) rotate(-25deg);
}
a.animated-button.victoria-two {
  border: 2px solid #D24D57;
}
a.animated-button.victoria-two:after {
  background: #D24D57;
  -moz-transform: translateX(-50%) translateY(-50%) rotate(25deg);
  -ms-transform: translateX(-50%) translateY(-50%) rotate(25deg);
  -webkit-transform: translateX(-50%) translateY(-50%) rotate(25deg);
  transform: translateX(-50%) translateY(-50%) rotate(25deg);
}
#container {
	margin: 0px auto;
  width : "640";
  height:"480";
	border: 10px #333 solid;
}
#divcontainer {
  position: relative;
	margin: 0px auto;
  width : "300";
  height:"300";
	border: 10px #333 solid;
}

#videoElement {
	background-color: #666;
}

/* Control the right side */
.right {
  right: 0;
  background-color:red;
}
.extra {
  width: 100%;
  height: 50px;
  top: 100%;
  background-color: firebrick;
  border-bottom-left-radius: 15px;
  border-bottom-right-radius: 15px;
  line-height: 24px;
  text-indent: 5px;
  font-size: 20px;
}
/* If you want the content centered horizontally and vertically */
.centered {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  text-align: center;
}

/* Style the image inside the centered container, if needed */
.centered img {
  width: 150px;
  border-radius: 50%;
}

.button {
  background-color: #4CAF50; /* Green */
  border: none;
  color: white;
  padding: 15px 32px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 16px;
}


.video {
  position: relative;
  z-index: 1;
}
.canvas {
  position: absolute; 
  top: 100px; 
  color: #FFF; 
  text-align: center;
  font-size: 20px;
  background-color: rgba(221, 221, 221, 0.3);
  width: 640px;
  padding: 10px 0;
  z-index: 2147483647;


}
#overlay {
  position: relative; 
  
}

#inputVideo {
  position: relative; 
}
</style>

<div class="jumbotron text-center">
  <div class="container">
    <h1>Advertisments based on human features recognized by AI </h1>
    <p style="color:#888;">Ahmed Elmahy</p>

    <button type="button" class="button" id="snap">Click here to Open Camera</button>
    <button type="button" class="button" id="play">Click here to run the models after you open the camera</button>

    </div>
  </div>


<div class="divcontainer">
    <div class="right">
        <h3 id="result_gender">Results</h3>
        <h3 id="result_glass">Results</h3>
        <h3 id="result_chubby">Results</h3>
    </div>
    <video  onplay="onPlay(this)" id="inputVideo" autoplay muted ></video>
    <canvas id="overlay"/>
    
</div>

</div>







<script src="https://code.jquery.com/jquery-1.12.2.min.js" type="text/javascript"></script>
<script src="{{ url_for('static', filename="js/face-api.js")}}"></script>

<script>
  //width:640;
//height:480
// onplay="onPlay(this)"
  // <div id="viewer">
  //       <div id="cam">
  //           <video style="display:block;" id="video"  width="640" height="480" autoplay></video>
  //       </div>
  //   </div>

$(document).ready(function() {


 })
const videoEl = document.getElementById('inputVideo')
const canvas = document.getElementById('overlay')

async function run() {
  faceapi.loadTinyFaceDetectorModel('https://www.rocksetta.com/tensorflowjs/saved-models/face-api-js/')

  const stream = await navigator.mediaDevices.getUserMedia({ video: true })

    //navigator.mediaDevices.getUserMedia( {video: true})
    //.then((stream) => {
  videoEl.srcObject = stream;

  //onPlay(videoEl)
  //console.log("result")


  //  }).catch((err) => {console.log(err);});


   
  //await faceapi.loadTinyFaceDetectorModel("{{ url_for('static', filename='models/')}}")
  //await faceapi.loadFaceRecognitionModel("{{ url_for('static', filename='models/')}}")
  //await faceapi.loadMtcnnModel("{{ url_for('static', filename='models/')}}")
  //await faceapi.loadFaceRecognitionModel("{{ url_for('static', filename='models/')}}")
  //await faceapi.loadFaceLandmarkTinyModel("{{ url_for('static', filename='models/')}}");

  //await faceapi.loadFaceLandmarkTinyModel('https://www.rocksetta.com/tensorflowjs/saved-models/face-api-js/')


  

  //const alignedFaceTensors = await extractFaceTensors(input, alignedFaceBoxes)

  //const descriptors = await Promise.all(alignedFaceTensors.map(
  //))



}
/* async function getFullFaceDescription(blob, inputSize = 512) {
  // tiny_face_detector options
  let scoreThreshold = 0.5;
  const OPTION = new faceapi.TinyFaceDetectorOptions({
    inputSize,
    scoreThreshold
  });
  const useTinyModel = true;

  // fetch image to api
  let img = await faceapi.fetchImage(blob);

  // detect all faces and generate full description from image
  // including landmark and descriptor of each face
  let fullDesc = await faceapi
    .detectAllFaces(img, OPTION)
    .withFaceLandmarks(useTinyModel)
    .withFaceDescriptors();
  return fullDesc;
} */
/* const mtcnnForwardParams = {
  // limiting the search space to larger faces for webcam detection
  minFaceSize: 200
} */
function resizeCanvasAndResults(dimensions, canvas, results) {
    const { width, height } = dimensions instanceof HTMLVideoElement
        ? faceapi.getMediaDimensions(dimensions)
        : dimensions
    //canvas.width = width
    //canvas.height = height

    return results.map(res => res)
    //result = results[0]
    
    //return result.detection.forSize(width, height);
     
};


function drawDetections(dimensions, canvas, results) {
    //const resizedResults = resizeCanvasAndResults(dimensions, canvas, results)
    //faceapi.drawDetection(canvas, resizedResults.map(det => det.detection))
    //faceapi.drawDetection(canvas, resizedResults)
    
    //return canvas

};


// function drawLandmarks(dimensions, canvas, results, withBoxes = true) {
//     const resizedResults = this.resizeCanvasAndResults(dimensions, canvas, results)
//     if (withBoxes) {
//         faceapi.drawDetection(canvas, resizedResults.map(det => det.detection))
//     }
//     const faceLandmarks = resizedResults.map(det => det.landmarks)
//     const drawLandmarksOptions = { lineWidth: 2, drawLines: true, color: 'green' }
//     faceapi.drawLandmarks(canvas, faceLandmarks, drawLandmarksOptions)
// };


async function onPlay(videoEl) {

  const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 128, scoreThreshold: 0.2 })

  var result = await faceapi.detectSingleFace(videoEl, options)

  console.log(result)

  if (result) {

    //const resizedResults = resizeCanvasAndResults(videoEl, document.getElementById('overlay'), [result])
//     if (withBoxes) {
    //drawDetections(videoEl, document.getElementById('overlay'),  [result])
    //var canvas2 = document.getElementById('canvas2');
    const resizedResults = resizeCanvasAndResults(videoEl, canvas, [result])
    console.log("resizedresults")
    console.log(resizedResults)
    cropped_box = resizedResults.map(resizedResults => resizedResults.box)
    dim_x = Math.round(cropped_box.map(box => box.x))
    dim_y = Math.round(cropped_box.map(box => box.y))
    dim_width = Math.round(cropped_box.map(box => box.width)) 
    dim_height = Math.round(cropped_box.map(box => box.height)) 

    
    var crop = {
      top :  Math.max((dim_y - 50),0),
      left : dim_x ,
      width : dim_width + 10,
      height : dim_height ,
    }


    //var c = document.getElementById("overlay");

    var ctx = canvas.getContext("2d");
    //canvas.width = crop.width;
    //canvas.height = crop.height;

    //ctx.rect(crop.left,crop.top, crop.width, crop.height);
    //ctx.stroke();


    //var canvas_cropped = document.createElement("canvas");
    console.log([ctx,crop.top,crop.left])

    //var ctx = cropped_can.getContext("2d"); // so we can draw
    //ctx.drawImage(videoEl, ,);
    
    canvas.width = 128
    canvas.height = 128
    //canvas.getContext("2d").drawImage(video, 0, 0, 300, 300);
    ctx.drawImage(videoEl, crop.left, crop.top, crop.width, crop.height, 0, 0, 128, 128);
    send_canvas_ctx();
    //send_canvas_ctx(canvas);
    //cropped_can = canvas_withleft_top[0]
    
   
    
//       $("#res").show();
}
  else {
    console.log("else")
    setTimeout(() => onPlay(videoEl),200)
  }


    
//     }
            //console.log(result)
            //drawLandmarks(videoEl, document.getElementById('overlay'), [result], true)
}

  //let fullDesc = await faceapi
  // .detectAllFaces(document.getElementById('inputVideo'))
  //return fullDesc;
  //const mtcnnResults = await faceapi.mtcnn(document.getElementById('inputVideo'), mtcnnForwardParams)

  //const detectionsArray = fullDesc.map(res => res.faceDetection)
  //faceapi.drawDetection('overlay',detectionsArray, { withScore: false })
  //faceapi.drawLandmarks('overlay', mtcnnResults.map(res => res.faceLandmarks), { lineWidth: 4, color: 'red' })
  //setTimeout(() => onPlay(videoEl),200)

  


//faceTensor => faceapi.computeFaceDescriptor(faceTensor)
//const alignedFaceBoxes = results.map(
 // ({ faceLandmarks }) => faceLandmarks.align()
//)


//const options = new faceapi.MtcnnOptions(mtcnnParams)
//const input = document.getElementById('inputVideo')
//const fullFaceDescriptions = await faceapi.detectAllFaces(input, options).withFaceLandmarks().withFaceDescriptors()


//<div id="res" style="cursor:pointer;height:30px;"></div>
//var video = document.querySelector("#videoElement");
// var video = document.getElementById('inputVideo');

// var ctx;
// var canvas = document.createElement('canvas');
// // Converts image to canvas; returns new canvas element
// canvas.width="640"
// canvas.height="480"


document.getElementById("snap").addEventListener("click", function() {
  run()
});
document.getElementById("play").addEventListener("click", function() {
  onPlay(videoEl)
});




// function ping() {
// 	window.fetch('/snap_a_signal')
// 	  .then(function(response) {
// 		      return response.text()
// 		    }).then(function(body) {
//           if (body["result"] == "0") {
//             console.log(body["result"] == "0");
//           } else {
// 				    console.log(body);
//           }
// 		    });
// }


function send_canvas_ctx() {
    input_data = canvas.toDataURL('image/png');
    //console.log(input_data)
	  window.fetch('/snap_a_signal', {
  		method: 'POST',
  		headers: {
    		'Content-Type': 'application/json'
  		},
  		body: JSON.stringify({
    		data: input_data
  		})
	}).then((response) => response.json()).then((json) => {
    document.getElementById('result_gender').innerHTML = json["gender"]
    document.getElementById('result_glass').innerHTML = json["glass"]
    document.getElementById('result_chubby').innerHTML = json["chubby"]
    
    console.log("else2")
    //const videoEl = document.getElementById('inputVideo')
    onPlay(videoEl)
  })
  
  }
          



</script>

</body>
</html>
